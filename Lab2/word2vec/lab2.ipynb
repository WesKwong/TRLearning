{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data_path, result_path, mode=\"hs_cbow\"):\n",
    "    data_path = os.path.join(data_path, 'text8')\n",
    "    train_cmd = f\"./word2vec -train {data_path}\"\n",
    "    hyperparam = \"-size 200 -window 5 -min-count 5 -threads 16 -iter 20 -binary 1\"\n",
    "    # HS_CBOW\n",
    "    if mode == 'hs_cbow':\n",
    "        hs_cbow_path = os.path.join(result_path, 'hs_cbow.bin')\n",
    "        os.system(f\"{train_cmd} -output {hs_cbow_path} -cbow 1 -hs 1 -negative 0 {hyperparam}\")\n",
    "        print(\"\\nHS_CBOW training done.\")\n",
    "    elif mode == 'hs_sg':\n",
    "        hs_sg_path = os.path.join(result_path, 'hs_sg.bin')\n",
    "        os.system(f\"{train_cmd} -output {hs_sg_path} -cbow 0 -hs 1 -negative 0 {hyperparam}\")\n",
    "        print(\"\\nHS_SG training done.\")\n",
    "    elif mode == 'ns_cbow':\n",
    "        ns_cbow_path = os.path.join(result_path, 'ns_cbow.bin')\n",
    "        os.system(f\"{train_cmd} -output {ns_cbow_path} -cbow 1 -hs 0 -negative 5 {hyperparam}\")\n",
    "        print(\"\\nNS_CBOW training done.\")\n",
    "    elif mode == 'ns_sg':\n",
    "        ns_sg_path = os.path.join(result_path, 'ns_sg.bin')\n",
    "        os.system(f\"{train_cmd} -output {ns_sg_path} -cbow 0 -hs 0 -negative 5 {hyperparam}\")\n",
    "        print(\"\\nNS_SG training done.\")\n",
    "    else:\n",
    "        print(\"Invalid mode. Please choose from 'hs_cbow', 'hs_sg', 'ns_cbow', 'ns_sg'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training using file ../data/text8\n",
      "Vocab size: 71291\n",
      "Words in train file: 16718843\n",
      "Alpha: 0.000005  Progress: 100.07%  Words/thread/sec: 119.28k  \n",
      "HS_CBOW training done.\n"
     ]
    }
   ],
   "source": [
    "train(\"../data\", \"../results\", \"hs_cbow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training using file ../data/text8\n",
      "Vocab size: 71291\n",
      "Words in train file: 16718843\n",
      "Alpha: 0.000002  Progress: 100.07%  Words/thread/sec: 50.20k  \n",
      "HS_SG training done.\n"
     ]
    }
   ],
   "source": [
    "train(\"../data\", \"../results\", \"hs_sg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training using file ../data/text8\n",
      "Vocab size: 71291\n",
      "Words in train file: 16718843\n",
      "Alpha: 0.036751  Progress: 26.50%  Words/thread/sec: 141.85k  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.000005  Progress: 100.07%  Words/thread/sec: 136.71k  \n",
      "NS_CBOW training done.\n"
     ]
    }
   ],
   "source": [
    "train(\"../data\", \"../results\", \"ns_cbow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training using file ../data/text8\n",
      "Vocab size: 71291\n",
      "Words in train file: 16718843\n",
      "Alpha: 0.021993  Progress: 12.03%  Words/thread/sec: 49.48k  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.000002  Progress: 100.07%  Words/thread/sec: 47.65k  \n",
      "NS_SG training done.\n"
     ]
    }
   ],
   "source": [
    "train(\"../data\", \"../results\", \"ns_sg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    similarity = dot_product / (norm_vec1 * norm_vec2)\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spearman_rank_correlation(x, y):\n",
    "    ranks_x = np.argsort(np.argsort(x))\n",
    "    ranks_y = np.argsort(np.argsort(y))\n",
    "    correlation = np.corrcoef(ranks_x, ranks_y)[0, 1]\n",
    "    return correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_word_vectors(name, path):\n",
    "    vocab_size = 0\n",
    "    vector_size = 0\n",
    "    word_vectors = {}\n",
    "    with open(os.path.join(path, name), 'r') as file:\n",
    "        # Read the first line to get the vocabulary size and vector size\n",
    "        line = file.readline().strip()\n",
    "        vocab_size, vector_size = map(int, line.split())\n",
    "        # Read the word vectors\n",
    "        for line in file:\n",
    "            word, *vector = line.strip().split()\n",
    "            word_vectors[word] = np.array(vector, dtype=float)\n",
    "    return word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_target_wordsim(path):\n",
    "    target_wordsim = []\n",
    "    with open(path, 'r') as file:\n",
    "        for line in file:\n",
    "            word1, word2, score = line.strip().split()\n",
    "            target_wordsim.append((word1, word2, float(score)))\n",
    "    return target_wordsim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(pred_path, target_path, name):\n",
    "    pred_word_vectors = load_word_vectors(name, pred_path)\n",
    "    target_wordsim = load_target_wordsim(target_path)\n",
    "    cosine_similarities = []\n",
    "    target_scores = []\n",
    "    for word1, word2, score in target_wordsim:\n",
    "        if word1 in pred_word_vectors and word2 in pred_word_vectors:\n",
    "            vec1 = pred_word_vectors[word1]\n",
    "            vec2 = pred_word_vectors[word2]\n",
    "            cosine_similarities.append(cosine_similarity(vec1, vec2))\n",
    "            target_scores.append(score)\n",
    "    spearman_correlation = spearman_rank_correlation(np.array(cosine_similarities), np.array(target_scores))\n",
    "    print(f\"Spearman correlation for {name}: {spearman_correlation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman correlation for hs_cbow.txt: 0.680260240495881\n",
      "Spearman correlation for hs_sg.txt: 0.7302820890449755\n",
      "Spearman correlation for ns_cbow.txt: 0.7043794202851639\n",
      "Spearman correlation for ns_sg.txt: 0.7483095696645032\n"
     ]
    }
   ],
   "source": [
    "pred_path = \"../results\"\n",
    "target_path = \"../data/wordsim_similarity_goldstandard.txt\"\n",
    "evaluate(pred_path, target_path, \"hs_cbow.txt\")\n",
    "evaluate(pred_path, target_path, \"hs_sg.txt\")\n",
    "evaluate(pred_path, target_path, \"ns_cbow.txt\")\n",
    "evaluate(pred_path, target_path, \"ns_sg.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
